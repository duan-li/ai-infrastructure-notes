

Foundation model training
Selecting transcript lines in this section will navigate to timestamp in the video
- Let's take a look at some of the decision-making factors involved when deciding whether or not to create a model yourself or to use a pre-trained model. If you want to create a model from scratch, that means you're going to need to design a new model architecture on a custom dataset that you're going to have to gather yourself. And this is, unfortunately, necessary if there's not an existing pre-trained model that could be used, or if you need an extraordinary amount of customization. Many workflows just don't need this level of effort. Or using a pre-trained model, this ends up being a lot more cost effective as well as time-efficient and resource efficient. And if you were to take an existing LLM or computer vision model, and then simply modify it further, that might reduce a vast amount of time in order to get to market. And so, a pre-trained model is simply the initial training phase using a vast data set, but it's on an existing architecture. And the purpose of this is just to learn the general patterns and the features from that diverse dataset. The benefits of this is that you don't need as many resources or time for downstream effort. It also gives you a great starting point for a lot of different application types. And then you can go further with fine-tuning. This is the process of taking a pre-trained model and then customizing it for a specific task or domain. And this involves more training of the model, but on a smaller task-specific data set that can take as short as even 15 or 30 minutes. The advantages of this is it allows you to take the flexibility of a pre-trained model, but then focus it for higher accuracy on a domain-specific use case. And it gives you the ability to balance the trade-off between resource investment and the performance of the model. And so, some decision-making factors here are how much do you want to invest to achieve the level of performance that you're looking for? This is where business value has to be considered. Do you have access to high-quality domain specific data? And if so, how much? If it's not a lot, you might want to consider fine-tuning rather than creating from scratch. And finally, the expertise requirements. Creating a model from scratch requires a large amount of domain expertise versus simply fine tuning or just using an existing pre-trained model.



Foundation model fine-tuning
Selecting transcript lines in this section will navigate to timestamp in the video
- There are different ways to execute fine-tuning against an existing foundation model. And so let's take a look at some of these methods. We're going to look at instruction tuning, adapting models for specific domains, transfer learning and continuous pre-training. So let's start with instruction tuning. What exactly is this? This is just retraining a model on the data sets of prompts and desired outputs. So this would require a document, usually JSON or some structured format where you have a prompt and then the output that you want the model to return. And you do this for a lot of different scenarios and you end up training the model that way. And this is going to help to improve the model's ability to understand and execute commands. And it's really effective for interactive applications. And so let's take a look at an example. So when Amazon was putting together new capabilities for Alexa, they use instruction tuning and they train the model on a data set that is made up of different user prompts and the corresponding desired action. So the the in instructions might be something like, "play my favorite playlist" or "Set a timer for 10 minutes." And so by doing this, they made it so that it was better able to understand and execute the different user commands. And this results in improved accuracy and customer satisfaction. Our next example is to adapt a model for a specific domain. And so this would be fine-tuning on a domain specific corpus like legal or medical. And the benefits here is that it's going to help with relevance and accuracy for specific tasks, and it helps to provide context aware responses. And I want to point out here that using a RAG, retrieval augmented generation, could also help in this specific context. An example of adapting a model for a specific domain would be ROSS intelligence, utilizing a foundation model that's trained specifically on legal texts. And that includes case law, statutes, legal opinions. And so by fine-tuning the model that way, they could accurately understand and analyze legal language. And this allows them to assist attorneys by answering legal questions, providing relevant case law references, and even suggesting legal strategies. And this is a great area to also point out that accuracy is important, but also reducing hallucinations to a minimum is critical. And combining this type of fine-tuning with retrieval augmented generation can be even more powerful. Our third example is transfer learning, where you just take a an existing pre-trained model and just use it for a new related task. And this is really efficient, even if you're working with a narrower scope, and it reduces the amount of additional training that's needed. And so our example here would be the initial training on BERT, one of the very first LLMs trained on a large corpus of just general text data. And so after that foundational training, they employ transfer learning to adapt for specific tasks such as sentiment analysis. And so when you take this model that's just designed for general text stuff and ask it to perform sentiment analysis, it was able to not only do that, you know, fairly well, but you can apply just regular fine-tuning beyond that so that you can improve the accuracy further. Now our fourth example is continuous pre-training. And this just means you're extending the training phase of the model with new data periodically over time. And what this does is it keeps the model updated with more current information and trends. It allows you to gradually impact things like bias or toxicity and helps that the output remains relevant and accurate. And the example here is X, employing continuous pre-training for recommendation algorithms. So the platform regularly feeds the models with new tweets and trending topics and user interactions so that the recommended content is more relevant to the end users and doesn't get stale over time.


Foundation model data preparation
Selecting transcript lines in this section will navigate to timestamp in the video
- There are several factors to consider when you get into the data preparation step for training a foundation model. The first of these is data curation. And there are a couple of subfactors here that we have to look at, including rigorous selection, meaning you need to make sure that you're carefully selecting the data that is directly relevant to the task or problems you're attempting to solve. This means getting rid of irrelevant but especially low-quality data that could impact performance. We also need to look at contextual relevance. Once the data has been curated, you still need to look through it again. Make sure it's representative of the specific domain. Make sure that it is indeed what you're looking for. And even if you want to fine-tune a model, you still need to do this two-step process to make sure that you're going to end up with an appropriate result. Next, we have governance and compliance. First, governance. You need to have a governance framework in place to oversee the whole data management process and something that scales to accommodate the amount of data. And this includes policies for data use, data storage, and data access. There's also regulatory compliance that if you are creating a model that might be bound by certain frameworks, GDPR, HIPAA, and so forth, that you are following all of the controls that have been laid out for that. Our next factor is labeling. And the first part of this is to come up with a clear labeling system, high-quality labels to ensure that each example of the data is properly tagged. And this is going to be really important for guiding the model's learning and improving its ability to be relevant. Next is the annotation accuracy. So, it might require involving subject matter experts in that labeling process to make sure that the labels truly are appropriate to match the data. Next, we have a couple of points that really deserve further discussion, like diversity in data. And this is making sure that your dataset is representative across different demographics, contexts, and scenarios within the domain in order to avoid perpetuating biases. And if there are biases that you can identify, there are techniques for mitigating them. And this might include adversarial training or further fine-tuning with counterexamples. Next, we have the size of the dataset. And this just has one subpoint, quality over quantity. If you want to fine-tune, especially that you need to make sure that that dataset is the highest quality that you can possibly get it, focus on something that's manageable, that gives you enough examples to capture nuances without overfitting. Next, we have reinforcement learning from human feedback, which is RLHF. And this means incorporating user or expert feedback into the training process. This could be a system where users can provide that real-time feedback, like a thumbs up or thumbs down, on the model's performance. There's also interactive training. Make sure that that feedback loop is returned back to the actual model, but also that the predictions are continuously evaluated against the human expectations.


Question breakdown, part 1
Selecting transcript lines in this section will navigate to timestamp in the video
- Welcome. In this question breakdown, we have a scenario that involves the fine-tuning of an AI model, and trying to figure out what kind of fine-tuning was actually performed. Let's go ahead and read the question. A legal AI system has been developed to assist lawyers in their research and case preparation. This system was trained specifically on legal texts, such as case law and statutes to improve its understanding of complex legal language. Which method of fine-tuning was primarily utilized in this scenario? We have four answer choices, Instruction Tuning, Adapting Models for Specific Domains, Transfer Learning, and Continuous Pre-Training. And so now we have to figure out which one of these was actually used in the scenario. Let's start with A, Instruction Tuning. Well, Instruction Tuning is just going to focus on training the model with prompts and desired outputs. That doesn't feel like what's happening here, where they've tried to tie it to a specific set of texts and language. B, Adapting Models for Specific Domains. And so this method is fine-tuning a model on domain-specific data, such as legal texts. Oh wait, that seems like exactly what we've done here with the purpose of enhancing performance and relevance. This is what they've done, but let's make sure that we can eliminate the other answer choices. C, Transfer Learning. That might ha be able to be okay here if you were to take just a general LLM, for example, and use it for legal purposes. You might be able to get some useful answers out of it, but it's not going to be specific enough to truly be relevant, and so Transfer Learning is not exactly what they did. And finally, we have Continuous Pre-Training, and this is going to involve updating the model over time periodically with new data. Now, this is something that might still need to happen, but it's not what they did initially by additionally training it on the domain-specific information, and so we can essentially eliminate this one as well, and that leaves us with Adapting Models for Specific Domains as the correct answer.



Question breakdown, part 2
Selecting transcript lines in this section will navigate to timestamp in the video
- In this practice question, we have a scenario that involves foundation model fine tuning, specifically for customer support. And so let's go ahead and dig into the question. You are preparing to fine tune a foundational model for customer support interactions to improve its performance and accuracy. The success of this process relies heavily on the quality of data that's used for training. Which step is most critical to ensure the model effectively learns from the provided data? And we have four possibilities here. Data curation, data size increase, initial training with diverse data, and unstructured feedback collection. Let's go ahead and go through the answer choices and see if we can either identify the right answer or eliminate everything else. A, data curation, well, this is critical in fine tuning. It means making sure that you have highly relevant examples that are specific to this customer support domain, and that will indeed help the model learn from the appropriate data and that is going to impact performance. We can't explicitly eliminate this answer, but let's see if any of the other answers are closer to what we're looking for. B, data size increase. Having enough data for fine tuning is important so that you can avoid under fitting. But fine tuning can often benefit by having a smaller data set that is higher quality rather than simply just adding more input. And this is just an emphasis on quality over quantity. And that doesn't feel like exactly what's happened here. C, initial training with diverse data, and this is something that might have already happened with the model, focusing on just broad generalization, diverse knowledge, being able to hold a conversation and have the appropriate tone. And that's great for the foundation model side of it, but fine tuning does require a more targeted approach. And so this doesn't seem like what's happened either. And that brings us to D, unstructured feedback collection. And this is actually a really important step to ensure that the model can be evaluated afterwards. Is it actually accurate? Is it providing business value? Is it useful? But that unstructured feedback is something that happens after the fine tuning has already been completed. And so that's not what's happened here either. And so we actually go all the way back to the first answer choice, data curation as the correct answer.