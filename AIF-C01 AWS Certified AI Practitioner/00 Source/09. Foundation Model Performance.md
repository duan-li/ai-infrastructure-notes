
Foundation model performance metrics and evaluation
Selecting transcript lines in this section will navigate to timestamp in the video
- There are a number of different performance metrics that can be used to evaluate how a foundation model is doing, especially when it comes to accuracy. And so we're going to take a look at a couple of these, and brace yourselves, there's going to be some interesting acronyms that we're going to be going through. And we're going to start with ROUGE. That stands for Recall-Oriented Understudy for Gisting Evaluation, but ROUGE is a lot easier. And this is a set of metrics for evaluating the summarization and machine translation primarily of LLMs. There are a couple of different components here. We've got ROUGE-N, which is for N-grams and ROUGE-L for longest common subsequence. And we're going to take a look at both of these coming up. And the purpose of this is to measure the overlap between the generated text and reference texts. And so a higher ROUGE score reflects a better quality summary. The definition of an N-gram is a contiguous sequence of N items, which are usually words, characters, punctuation from a given text or speech. So let's take a look at an example. We have a news article about a recent scientific discovery about a new drug. The original article is pretty detailed. We would like to generate a concise summary. And so let's take a look. We have a snippet out of the original article, which has a decent amount of text, and it's fairly complicated talking about the new drug and clinical trials. And there are some important points in that text. And let's say we generate a summary, and the summary says researchers at XYZ University discovered a new drug that reduces tumor size significantly in cancer patients. And you can see that's a lot shorter. It's a single sentence, it's simple language. So if we want to evaluate the performance of the model, let's take a look at ROUGE-N to start with. And this is where we look for common unigrams, and we're looking for one unigram at a time. So that's why it says ROUGE-1. And we do have some common unigrams. New, drug, researchers, XYZ, University, reduced, tumor, size, cancer. And if that generated summary has 10 total unigrams and eight of them are overlapping with the original article, then our overall score is 8 out of 10, 0.8, that's 80%. We can also do ROUGE-L, and that's the longest common subsequence. And so with our current example, we have sequences like new drug, researchers, XYZ University, and reduced tumor size. So if we have say five words in the longest common subsequence, then the score would reflect the proportion of that reference summary that was covered. Another type of performance evaluation we can do is BLEU, Bilingual Evaluation Understudy. And so this is a metric for evaluating when you're trying to translate from one language to another by comparing the N-grams. Now there is a limitation here that if you have an overly short translation that it's going to back off from doing it entirely. And so our purpose here is to see how many of the N-grams appear in the generated text versus a reference translation of the original text. So an example here is a situation where we have a sentence in English that needs to be translated to Spanish, and our original English sentence is the cat sits on the mat. And then we have our reference Spanish translation, (speaks Spanish). And the generated translation is not exactly the same. It's pretty close, but it's not exact. And so if we want to evaluate this, we look at our reference translation with all of the N-grams, and then we look at the generated translation and all of its N-grams. And so we look for the common unigrams, which is just one N-gram, and we can identify that three out of seven are in the reference translation from the generated translation. And so that helps us to identify that our total score would be just under 43% as far as the precision is concerned. Another score that we can use is called BERTScore. And this is a metric that leverages BERT embeddings to evaluate similarity of text. And so this is going to measure the similarity, not just by doing essentially keyword matching, but looking at it from a more semantic level. And so this is going to measure how many N-grams in the generated text appear in the reference translations, but looking at similarity rather than exactness. So we have a scenario, we have an English sentence that we'd like to summarize, and we want to evaluate how well the generated summary captures the meaning of the original sentence, the meaning, not the exact words. So our original sentence, artificial intelligence is transforming various industries by enhancing efficiency and enabling new capabilities. Sounds like a bunch of buzzwords honestly. Our reference summary is AI is improving industries by increasing efficiency, that's a lot better. The generated summary is pretty close to that. Artificial intelligence enhances efficiency in different sectors. And so with the BERTScore evaluation, we generate text embeddings for both. We compute the similarity between them by doing that vector similarity calculation. We calculate the precision and recall from there. And then finally we generate what we call an F1 score. And so why do we evaluate the performance of a model? It makes sure that the models are performing as expected. We get to determine whether or not there are strengths and weaknesses. Maybe there are certain tasks that they do very well on and others that they don't. And finally, it allows us to align the technology with our strategy. Now from a business objective perspective, we do this to improve decision making, and the customer experience, as well as driving innovation. So as we dig a little bit deeper, in order to evaluate how a model is performing, we can do this by just using humans. We have a human assessor interacting with the model where we do things like open-ended conversations, answering questions, text generation, just you know, talking to the model. And our criteria is, it's hard to quantify sometimes, coherence, relevance, factuality, overall quality. But again, it's a human who's evaluating that. But if it's a human who's going to be using the model, this is not necessarily a bad way to go. So the advantage here is this would be to set the gold standard for a quality assessment. But our challenge is that you have a human who has to perform the work. That's time consuming and it's expensive, especially if you trying to scale it. And so there are different benchmark datasets that can be used. And these are just curated collections of data for performance evaluation that give you the ability to provide standardized tasks and metrics for assessment. And there are a number of different benchmark datasets out there. And we're going to, we'll take a look. We've got GLUE, which focuses on language understanding, SuperGLUE, which is kind of a hyped up version of that. We've got SQuAD, which evaluates question answering, and then there's WMT for evaluating machine translation. And the advantages of using these, one, is that you're able to standardize so you can get an objective value rather than just a human, and the subjective nature of that. You've got coverage, because it's already determined the different topics that need to be addressed, and you can track progress, which enables the ability to monitor the change over time in the quality of the model.



Foundation model business objective criteria
Selecting transcript lines in this section will navigate to timestamp in the video
- How do we determine if an AI model is delivering business value? Well, there are a number of objectives and considerations that we should take a look at to be able to do this. The first is just whether or not the result aligns with business objectives. Does it address specific goals? We can use performance metrics to find KPIs that can be used to measure whether or not business value is being delivered. We can solicit user feedback, qualitative insight from the end users and we can evaluate how well the model has integrated into the existing workflows without causing other issues and so how do we evaluate productivity? We have an example of a customer support AI chatbot. Well, some metrics we could use to help with this is the average response time and ticket resolution rate. We could even use sentiment analysis on the conversation as a sort of metric and the analysis for this would be to compare the the pre-AI model metrics with the post-AI model metrics to assess the impact on overall productivity. Next is to enhance user engagement. An example here would be a product recommendation engine for e-commerce and some metrics that we could look at here include click-through rates, average session duration and conversion and so our analysis is just to analyze the user behavior before and after the deployment. Next we have task engineering, such as automating data entry processes using a foundation model. So our metrics here can include the time taken to complete the task and the error rate in data entry and the analysis would be not only to measure improvements in efficiency and accuracy over time, but to also compare how the previous process was working, especially if it was humans doing the work. You know, it's not always the end goal to reduce error rates to zero, it's just to reduce them lower than what it was when a human was doing the work.


Question breakdown, part 1
Selecting transcript lines in this section will navigate to timestamp in the video
- In this question breakdown, we are going to look at a scenario that deals with performance evaluation of an AI model. And so let's go ahead and read the question. During the evaluation of machine generated summaries, a new metric has been introduced that assesses the semantic similarity between the generated text and a reference summary using deep contextual embeddings. Which characteristic distinguishes this metric from traditional n-gram based evaluation methods? We have four answer choices that describe a bit of what might possibly be the difference. And so we're going to now need to go through these and evaluate, can we come up with the right answer or can we evaluate everything else so that the right answer is all that's left? A, it computes a score based on the exact matches of phrases in the text. Well, this would be more of a description of traditional methods like rouge or blue, which do exactly that. And that doesn't include semantic similarity, so we can probably eliminate this answer. B, it utilizes word embeddings that capture the meaning of the words in context. And this would be something more like generating a BERT score using contextual embeddings, and that allows it to assess the semantic similarity of the generated text versus reference text. And that sounds pretty close to what we're actually doing here. And so let's kind of tag this answer as it might be right, we can't eliminate it yet. C, it focuses solely on the length of the generated text compared to the reference. Well, length comparison really isn't a characteristic of semantic similarity. It's more about the meaning of the text instead. So we can eliminate this one. And our final answer D, it requires multiple reference summaries for a valid evaluation. While that could really help us with coming up with a better overall evaluation, that's not what's going on here. And this isn't addressing semantic similarity either. And so we can probably eliminate this one. Now having eliminated A, C and D, the only answer left is B, which is the correct answer.


Question breakdown, part 2
Selecting transcript lines in this section will navigate to timestamp in the video
- In this practice question, we have a scenario where we're going to attempt to determine the business value being delivered by an AI model. A company has recently implemented a foundation model to enhance its customer support services through an AI chatbot. Which of the following metrics should the company primarily focus on to evaluate the productivity improvements resulting from this implementation? And that's where the business value comes in. So we have four different choices: customer satisfaction scores, number of new customer inquiries, employee training hours, and average response time. And remember that this is in the context of an AI chatbot. And so let's walk through these choices. Customer satisfaction scores and this is important, and this can measure the user sentiment rather than direct productivity. It could be potentially a different type of business value, but it could also be affected by a lot of different factors, not just the chatbot itself. B, our next answer choice is the number of new customer inquiries. And this metric really doesn't evaluate the performance of the foundation model or the productivity of the chatbot itself, so we can eliminate this one as well. Employee training hours. This one should be an easy one to eliminate. This is entirely unrelated to the performance of the foundation model. We were looking for a customer support chatbot, not necessarily anything that has to do with employees at this point. That brings us to D: average response time. Now, this is something that we could actually compare to before the model was deployed and after the model was deployed. And it directly reflects the efficiency of the chatbot in handling customer support requests. And if this time goes down, that should indicate business value is being delivered. And so our correct answer is indeed D.