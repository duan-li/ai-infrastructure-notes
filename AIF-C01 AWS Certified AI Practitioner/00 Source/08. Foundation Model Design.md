
Pretrained model selection criteria
Selecting transcript lines in this section will navigate to timestamp in the video
- Instead of defining a number of different foundation model selection criteria, we're going to follow the experience and scenario of GlobalTech, a fictional company. They're planning to develop a multilingual customer support chatbot. The decision-making process is going to involve evaluating different pre-trained foundation models and looking for certain specific properties that will meet their business objectives. The first criteria is cost. They have budget allocated for the project and they take the time to compare different pre-trained models. They consider licensing fees and the computational costs of running these models. And what they find is that some of these models are too expensive, others fit within their budget, and they meet their needs without compromising performance. Our next criteria is modality. This is a chatbot, it's going to interact using text, and that means that GlobalTech can then focus on text-only models. They're going to explore options that can understand and generate human-like responses and making sure the chosen model excels in those text-based interactions in a conversational tone. Our next criteria is latency. And for this chatbot, they want to optimize performance as much as possible. And so, GlobalTech is going to prioritize models that exhibit low-latency, and they test different models, they find that some, while they're really, really accurate, have longer response times and that's not going to meet the real-time interaction requirements. This next criteria is an important one, and that is multilingual support. GlobalTech has a diverse customer base over different countries, so they require a model that can translate between multiple languages. And so, they're going to shortlist models that are known for multilingual capabilities, ensuring that they can support all the languages that are required. Now, as a side note, GlobalTech could also consider a standalone model that only does translation while picking a separate model for the actual conversation. Next is model size. And because the company's IT infrastructure is a bit limited, they do have to consider the size of the model. They look at memory, CPU, GPU, storage requirements of hosting these models locally, and they opt for one that balances performance with manageability because overly large models are going to be slow based on the amount of resources that GlobalTech can throw at the problem. Next is complexity. And this is important. The more complicated models can also give better accuracy, but they require a little bit more tuning, longer training times, and just more resources in general. And so GlobalTech's going to decide to go with a moderately complicated model, gives a good trade off between performance and ease of deployment. Next, we have customization. We do want to be able to tailor or customize the chat bot responses to align with the brand voice of the company. And so we're going to look for a model that allows for customization and fine tuning, and prioritize a model that support the retraining with your own data so that you can improve the relevancy and accuracy of interactions. And finally, input/output length. This is going to be looking at how long the requests and the responses are going to be, as well as how long the entire conversation chat is going to be in terms of tokens. They need a model that can handle detailed queries, give comprehensive answers, maybe even entire documents worth of answers, so they're going to verify that the chosen model can process and generate longer text sequences without issues like having it cut off in the middle. And so overall, the recommendation is going to be a mid-sized model that is moderately complex, text-only, there's no need for any other modes, multilingual, rather than going with two separate models, low latency, where it is reasonable. And finally, it needs to be customizable.



Model inference parameters
Selecting transcript lines in this section will navigate to timestamp in the video
- I'd like to cover some inference parameters that can be provided to foundation models at runtime to affect the output in desirable ways. Now, our first parameter is based around randomness and diversity, and it's called Temperature. Temperature controls the likelihood of selecting outputs. If the temperature is low, it's going to lead to more determinism, which means higher probability tokens, less variability in the output. A high temperature is going to lead to more diversity using lower probability tokens in the output. Our next parameter is called Top K, and this is going to define the number of candidate tokens that are considered for output. And here, a low number is going to limit the response to the most likely outputs only. Whereas a high number, it will include the less likely options. Our next parameter is called Top P, and this is the percentage of likely candidates considered, instead of just a discreet number. And similar to the others, a low number here will reduce diversity. A high number here increases the number of options that can be selected from. And so you can use all three of these parameters together to influence the output. And let's look at an example. Our prompt is, "The sky is filled with," with what? Well, if we feed this to a foundation model, we might determine that the candidates are going to be stars with a 0.6, clouds with a 0.3, and dragons with a 0.1. And so if we define a high temperature, that's going to increase the chance of dragons, it adds creativity. If we have a Top K = 3, that means that it can include all three options. Now, if we change this number to 2, stars and clouds, where we don't consider dragons at all, and kept the high temperature, all we've done is increase the chance of clouds being in the response. Finally, we have Top P = 0.8, and we only have three possibilities to choose from. What this means is that the stars and clouds are going to be likely, but it still allows dragons to have a chance of being included in the output. Now finally, another parameter that we can impact is output length. And this is where you specify an exact length for minimum or maximum tokens for the generated response. And there are penalties to potentially doing this, like the response length, the number of repeated tokens, the token frequency, the token type, all of those can impact the output length. And there's a lot of overlap here with just good old prompt engineering where you specify in the prompt rather than as a parameter, "Hey, I'd like this output to be three sentences or less," that will also impact things like output length.

Introduction to RAG
Selecting transcript lines in this section will navigate to timestamp in the video
- If we want to understand RAG in the context of generative AI, first we need to expand the acronym and then provide a short definition. RAG stands for retrieval augmented generation, and this is the process of augmenting LLM output by referencing a knowledge base that exists outside the context of the LLM training source. Now, there are a number of different technologies we can use to implement this, called knowledge bases. The first of these is just a traditional relational database or indexing system, like Elasticsearch. And so here you have documents that are indexed based on keywords or phrases. And then the retrieval process is usually implemented using a keyword search to identify documents that match the terms. And then those are sent off to the LLM for generating a response. Next, and this is a more common implementation, is to use a vector database. And this is where you take structured or unstructured text and you split it up into chunks, and the process is called chunking, and then you embed those chunks into vectors. And a lot of times this is done using a transformer-based encoding model. And then you store these vectors in the vector database and it allows you to, rather than doing keyword searches, you can now do similarity searches. And when you submit a prompt, you can search the database using a vector that represents a query, pulling the most relevant documents that match that similarity search, and then add that to the prompt. You can also have a hybrid knowledge base. And this is going to combine keyword searches and similarity searches. Sometimes one of them is performed before the other, other times, they're performed in parallel. And then if you use the right technology, then you have the ability to take the responses and re-rank them according to whether or not they showed up in both searches, for example. Now, another common implementation pattern is called graph RAG, which is a combination of the vectors in a graph database. And this is going to use structure-aware chunking, and it returns not just the closest chunk using the vector similarity search, but it also identifies and it extracts closely related chunks in the context of the knowledge graph to give a little bit more context. We can also just do direct model integration. And this is not really RAG anymore because all you're doing is retraining or fine tuning the model by including all of the context rather than passing it along with a prompt. And there are different applications for RAG, like building an intelligent question-answering system. If you have all of your document sources available as context, this can be a fantastic way to implement customer support and virtual assistants. You can also use 'em for expanding and enriching existing knowledge bases. You can do things like rephrasing and summarizing and highlighting key points, and you can use 'em for generating content. Summaries also belong in here if you want to do comparisons, if you want to generate reports. The benefits of using RAG include improved accuracy, that's one of the biggest ones, but also the ability to identify the source for the response, which enhances transparency and explainability. You get the contextual relevance, so you get to decide which documents are going to be the ones that it draws from, and you get better handling of specific verticals. So if you have all of your documents under a specific topic, it makes it easier to get improved results using that. Now, there are some challenges. It's going to increase the complexity of any pipeline that uses RAG. It's going to increase latency because you're also doing the vector search in addition to the LLM query, you are going to be dependent on the quality of the retrieval set that's pulled out of the RAG before being sent off to the LLM. And there are extra resource requirements because now you have to host a a graph database or some other knowledge base. And finally it's going to make tuning and maintenance a little bit more difficult, a little bit more complex, a little bit more expensive.



Introduction to vector databases
Selecting transcript lines in this section will navigate to timestamp in the video
- Let's talk about vector databases. But in order to do that, we need to define some terms, starting with vector database. This is a specialized data storage system that's designed to efficiently store index and retrieve high dimensional vector representations of data, which can be called vector embeddings. And this enables fast similarity searches and nearest neighbor queries. Next, we need to talk about a vector embedding. This is a dense numerical representation of data points called chunks, which are a contiguous string of single characters, words all the way up to sentences or longer, in a continuous vector space. And it captures the semantic relationships and similarities among the data in a way that facilitates ML and information retrieval tasks. Our next term, well, I mentioned chunks, so what is document chunking? This is the process of taking a large document and breaking it down into smaller, more manageable pieces. Each of those are called a chunk. Chunk size in terms of tokens, as well as chunk overlap, those are both calculated in token units, but there are critical metrics in order to tune a vector database properly. And so let's take a look at a vector embedding example. And we're only going to do this in two dimensions. We're not going to do the multi-dimensions that are actually in the technology, but this is something that can help to explain how the technology works. So we have some words, and I've already pre grouped them together a bit. Dog, wolf, fox, orange, lemon, banana, golf, basketball, cricket. So we have these words and we call these chunks, and we use an algorithm to generate a vector embedding. And so this vector is usually generated using an LLM, a transformer-based model, to identify the semantic similarity between the words. And so now we have a bunch of 2D vectors, which I'm representing in terms of Cartesian coordinates, which make it easy to visualize. So we can now graph these on a coordinate plane. And you can see that when you draw a line with an arrow from the origin to each of the points, those are the vectors that we are representing and they go off in all directions. But if you also noticed, they're kind of grouped together as well. And because they're grouped together, we can now organize them by similarity using those vectors, and we can then go back to our original set and organize them according to mathematical similarity. Now, if we want to execute a similarity search on the word fruit, we're not doing a keyword search, we're doing a similarity search, which means we need to generate a vector embedding for this word as well. And we do that and we end up with the coordinates four, four. Well, if we go ahead and graph this on this plane, we'll find that it's right there in the midst of the other fruits, but, we can see that it's equidistant from both orange and lemon, but a little bit farther away from banana. And so depending on our search parameters where we look at temperature, top P or top K, we might end up with orange and lemon being returned with equal probability, or banana or all three, depending on the type of query that we had issued.



AWS vector database service
Selecting transcript lines in this section will navigate to timestamp in the video
- AWS has several different offerings that can be used as a vector database store. And let's take a look at each of these offerings, starting with OpenSearch. This is a realtime search against text data and indexes. It's also just managed open-source OpenSearch software with a lot of different options. You can deploy this onto discrete servers, you can also deploy it serverless. It can generate embeddings externally, as long as you specify where they need to be generated. And then it stores and allows for vector-embedding search within the text and the indexes. Next, we have Aurora, but only for Postgres. This is just a cloud-native relational database service. It's just kind of the general explanation of it. It's also managed Postgres that has been refactored to be cloud native. And it too has server-based as well as serverless options. And it can be used to store and search the vector embeddings, but it doesn't have any built-in features for automatically generating the vector embeddings when the data is inserted in the first place. It does use an existing plugin for Postgres called pgvector. Next, we have RDS, also for Postgres. And this is going to be similar to Aurora, but instead of being cloud native, this is really just a traditional platform as a service offering as a relational database running Postgres. This does not have a serverless offering like the others. It also allows you to store and search the vector embeddings, but not generate them, and it too uses pgvector. Our next option is Neptune. And this is the AWS Managed graph database service, NoSLQ. And you've got a number of different client choices, which include Gremlin and Sparkle. This too has server-based and serverless options, like Aurora, like OpenSearch. And it can be used to store and search the vector embeddings, but doesn't have any ability to generate them internally or externally. And finally, we have DocumentDB. This is the AWS NoSQL document store. And it's really just managed MongoDB, or is very compatible with MongoDB. This is server-based only, kind of like RDS. And it is also used to store and search the vector embeddings. Now, I think, it's important to note that across all of these different offerings, only one of them, OpenSearch, has the ability to generate the vector embeddings, even if it has to do it externally. None of the others have the ability to do that, which means you have to have generated the vector embeddings first before inserting into the database service.



Foundation model customization cost tradeoffs
Selecting transcript lines in this section will navigate to timestamp in the video
- When you are considering the customization of a foundation model, there are different ways to do it, and each of those methods have cost trade-offs that are associated with them. Let's start by looking at pre-training, basically creating your own model. There's a very high computational cost, which means you're going to need a lot of resources, compute resources, GPUs, TPUs, and time. There is a data requirement. You need a massive amount of high-quality training data that has been pre-processed appropriately. You need expertise, deep expertise in model architecture and training methodologies. And on the other side of this, you can potentially get the highest overall performance, accuracy and so forth by going this route. Next, we have fine-tuning. This is a moderate computational cost. It's going to be less expensive than pre-training, still requires access to GPUs or TPUs. There's a much lower data requirement. In fact, it can be very small indeed and still get effective results. There's a much quicker turnaround. You could fine-tune a model in as short as 15 or 20 minutes. There is also a risk of overfitting. If your fin-tuning dataset is too small or not representative, you end up running into overfitting, which means that the resulting model has essentially memorized the dataset and is not able to do effective in inference on new data. Next, we have in context learning, and this would be the equivalent of providing extra context in the middle of a conversation or having the LLM learn in the middle of a conversation. There's low computational cost for this, doesn't require any additional training. It's very flexible. You can always start over. You can adjust the model's behavior by saying different things, but the ability to customize is limited. It's going to be certainly affected by the model's bias and guardrails and quick deployment. You just do this in the middle of a conversation. Our last option is RAG, retrieval augmented generation. This has a moderate to high computational cost, but not necessarily at any one time. When documents are inserted into the vector data store, that's going to require some cost. It's also going to add some latency to any similarity searches. Now you have to pay for data management. You're going to need a vector data store for all of this information. It can improve the performance. It's going to affect the accuracy by providing contextually relevant information so you get better responses. And finally, you get increased complexity because now you have an extra piece of your infrastructure, an extra tier that has to be part of the workflow.



Generative AI agents
Selecting transcript lines in this section will navigate to timestamp in the video
- What is an agent in the context of generative AI? Well, first, let's look at a generic definition. An agent would be a software component that is designed to perform specific actions autonomously or semi-autonomously based on some predefined rules. They're fairly static in their function, but in this case, we're going to use generative AI in agent software to accomplish tasks that are not easily performed by code that has rigid or predefined rules. And so let's take a look at a scenario. A company is developing an application which performs the curation of news articles for a specific list of companies and wants to use agents to accomplish tasks as appropriate. And here are some task possibilities that either an agent or a generative AI agent can take over. We need to generate a list of recent articles for each company. We need to evaluate the article source against a deny list that we've created previously. We need to calculate the article length, then calculate the relevance of the article according to the company we're trying to target, because a lot of articles will mention multiple companies. We want to make sure that it really is relevant to the one we're targeting at the time. We want to generate a summary of the article and want to calculate the sentiment of the article as well. Now, out of all of these tasks, generating a list of recent articles, evaluating against against deny list, those are easy for a regular agent. Calculating article length could be done by just looking at the number of characters, but what if we wanted to intelligently remove irrelevant text that doesn't belong there, header and footer information, for example, contact information that just don't matter in the context of the article? For that, we could use generative AI. So let's take a look at an infrastructure that has been deployed into AWS to do exactly this. We start off by configuring a scheduler for the required interval, you know, whether we're going to run this hourly, daily, weekly, or you know, whatever we choose. And on that schedule, we invoke a function running code, Python, or whatever. We'll call this the job initiator. And this is the target of the schedule, and this job initiator, the very first thing it's going to do is extract a company list and the new source deny list from a parameter storage service. From there, it loops through the list of companies and reaches out to the destination URLs and identifies any new articles for those companies. It will ignore any article from the deny list, and it may reach out using generative AI to calculate the length, or it just might grab the contents of the article and do a quick character count to see if it's over or under. Now, for the articles that have been determined to be worthy to continue, those are submitted as company and article metadata as a single message in a message queue. Then we have another function, and this one is triggered by that message queue. And it will take each of those articles and evaluate the article relevancy using an LLM, and you could calculate that relevancy on, say, a percentage basis or zero to one. And then you'd set a threshold that if the article calculated relevancy is less than a certain number, we just drop the article on the floor. Recognizing that an LLM is non-deterministic, you could feed the same article into the LLM multiple times, and you're going to get different relevancy numbers, and so you need to account for this in your thresholds. For the articles that pass the threshold requirement, you submit those articles as single messages to a second message queue, which is consumed by another function. And this one is going to reach out to an LLM to generate both a summary of the article, as well as perform the sentiment analysis. And from there, you place the results, the article metadata, the relevancy, the summary, and the sentiment score and explanation into, in this case, a NoSQL key value store that can be accessed directly by the user interface.



Question breakdown, part 1
Selecting transcript lines in this section will navigate to timestamp in the video
- In this question breakdown, we have a scenario that involves model inference parameters. Let's go ahead and read the question. "You are tasked with generating creative text using an AI model and need to set the inference parameters to encourage diverse outputs. If you choose a high temperature setting, what is the expected effect on the generated response?" We have four choices here, so we're going to want to look through these and see if we can, hopefully, eliminate incorrect answers so that what we have left is the correct answer. So let's go ahead and walk through these. We'll start with A. The model will produce repetitive and predictable responses. Well, a high temperature actually promotes variability rather than predictability, so this isn't right. B, The model will generate diverse and creative responses. Well, that's exactly what a high temperature is going to promote, randomness, creative responses, and really what it's doing is it's increasing the non-determinism of the response. This might be our correct answer. We still need to eliminate everything else, though. C, The model will ignore the prompt entirely. Well, the model will always consider the prompt, regardless of the temperature, regardless of bias, regardless of guardrails. At the very least, it's going to parse it. And finally, we have D, The model will only provide the most likely responses based on input. Well, this answer choice, this would be the expected outcome if we had chosen a low temperature instead of a high temperature. And that pretty easily eliminates everything else, so our correct answer is B.



Question breakdown, part 2
Selecting transcript lines in this section will navigate to timestamp in the video
- In this practice question, we are going to look at a scenario that involves customizing an AI model in the context of customer support. So here's the question. A tech startup is looking to deploy an NLP, natural language processing model, tailored for customer support. They want a solution that minimizes costs while allowing for rapid implementation and flexibility in adjusting the model's responses based on specific queries. Which customization approach should they choose? And we have four options here. Pre-training, fine tuning, in-context learning, and RAG, which stands for retrieval-augmented generation. Let's walk through these and see if we can identify the differences between them so that we can determine whether or not they meet the requirements. Pre-training. Pre-training is when you build the model in the first place. You can get a highly specialized model out of this, but it's resource-intensive, it's time-consuming, and that's not necessarily what we're looking for here. We want rapid implementation. B, fine-tuning, and this is a little bit closer to what we're looking for. We get to use this to improve the model's performance on customer support, but it still involves a certain amount of computational cost, as well as a moderately sized domain-specific dataset. So it's better, but not exact. C, in-context learning. Well, this is where we take conversations, and we take advantage of the fact that the LLM can adjust in the middle of that. And this gives us the ability to provide examples or prompts at inference time without any extra training, which makes it cost-effective and flexible. Also allows for rapid adjustments if you learn that you're not doing something in a way that turns into an actionable or valuable result. This is a lot closer than B. D, RAG, retrieval-augmented generation. This is going to combine retrieval with generation so you get better contextual responses, which can improve accuracy. It does introduce complexity, there's additional cost, and it's not necessarily designed around rapid implementation. And so out of these four answer choices, in-context learning is the most appropriate to meet the requirements.